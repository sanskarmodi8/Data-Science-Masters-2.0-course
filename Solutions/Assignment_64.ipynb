{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825d1409-c989-41b0-9e3a-c5b6b54dbacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is Random Forest Regressor?\n",
    "# Random Forest Regressor is an ensemble machine learning algorithm that extends the idea of bagging to decision trees for regression tasks. It builds multiple decision trees during training and aggregates their predictions to provide more accurate and robust regression predictions. Each decision tree in the Random Forest is constructed using a random subset of the training data and random subsets of features.\n",
    "\n",
    "# Q2. How does Random Forest Regressor reduce the risk of overfitting?\n",
    "# Random Forest Regressor reduces the risk of overfitting through two main mechanisms:\n",
    "\n",
    "# Random Subset of Data: Each decision tree in the ensemble is trained on a random subset of the training data, which introduces diversity and helps prevent overfitting to specific instances.\n",
    "# Random Subset of Features: At each split of a decision tree, a random subset of features is considered for the best split. This reduces the likelihood of individual trees fitting to noise in the data.\n",
    "\n",
    "# Q3. How does Random Forest Regressor aggregate the predictions of multiple decision trees?\n",
    "# Random Forest Regressor aggregates predictions using the mean (average) of the predictions made by individual decision trees. For regression tasks, each decision tree predicts a numerical value, and the final prediction from the Random Forest is the average of these individual predictions.\n",
    "\n",
    "# Q4. What are the hyperparameters of Random Forest Regressor?\n",
    "# Random Forest Regressor has several hyperparameters that can be tuned to optimize its performance. Some key hyperparameters include the number of trees (n_estimators), the maximum depth of trees (max_depth), the minimum number of samples required to split an internal node (min_samples_split), the maximum number of features considered for a split (max_features), etc.\n",
    "\n",
    "# Q5. What is the difference between Random Forest Regressor and Decision Tree Regressor?\n",
    "\n",
    "# Ensemble vs. Single Tree: Random Forest Regressor is an ensemble of multiple decision trees, whereas Decision Tree Regressor is a single decision tree.\n",
    "# Variance and Overfitting: Random Forest Regressor reduces variance and overfitting by combining multiple decision trees, while Decision Tree Regressor can easily overfit the training data.\n",
    "# Prediction Aggregation: Random Forest Regressor aggregates predictions by averaging them, while Decision Tree Regressor directly outputs a single prediction from a single tree.\n",
    "\n",
    "# Q6. What are the advantages and disadvantages of Random Forest Regressor?\n",
    "# Advantages:\n",
    "\n",
    "# Reduced Overfitting: Multiple trees and randomness reduce the risk of overfitting.\n",
    "# Better Generalization: Aggregation improves generalization to new data.\n",
    "# Robustness: Works well with various types of data and handles missing values.\n",
    "# Feature Importance: Provides feature importance scores.\n",
    "# Disadvantages:\n",
    "\n",
    "# Complexity: Training and prediction times can be longer than single decision trees.\n",
    "# Less Interpretability: Interpretability decreases as the number of trees increases.\n",
    "# Hyperparameter Tuning: Requires tuning of hyperparameters for optimal performance.\n",
    "\n",
    "# Q7. What is the output of Random Forest Regressor?\n",
    "# The output of the Random Forest Regressor is a predicted numerical value. For each input, it aggregates the predictions of multiple decision trees and provides the average of these predictions as the final output.\n",
    "\n",
    "# Q8. Can Random Forest Regressor be used for classification tasks?\n",
    "# Yes, Random Forest can be used for both classification and regression tasks. When used for classification, it's known as Random Forest Classifier. The key difference is in how the individual decision trees' predictions are aggregated: for regression, it's the average of predictions, and for classification, it's based on majority voting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04078314-1843-4ae1-857d-4a86bdcd0976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval:\n",
      "[14.43770528 15.54653198]\n"
     ]
    }
   ],
   "source": [
    "# Q1. What is an ensemble technique in machine learning?\n",
    "# An ensemble technique in machine learning is a method of combining multiple individual models (learners) to create a stronger, more accurate model. The idea behind ensemble techniques is to leverage the diversity and strengths of different models to improve overall predictive performance. Ensembles can be used for classification, regression, and other machine learning tasks.\n",
    "\n",
    "# Q2. Why are ensemble techniques used in machine learning?\n",
    "# Ensemble techniques are used to improve the generalization and robustness of machine learning models. By combining multiple models, ensemble techniques aim to reduce overfitting, enhance prediction accuracy, and handle complex relationships in the data that a single model might struggle to capture. Ensembles are particularly effective when individual models have different biases or strengths.\n",
    "\n",
    "# Q3. What is bagging?\n",
    "# Bagging (Bootstrap Aggregating) is an ensemble technique where multiple instances of a single machine learning algorithm are trained on different bootstrap samples of the training dataset. Each model's predictions are then combined by taking a majority vote (for classification) or an average (for regression) to make the final prediction. Bagging helps reduce variance and enhance model stability.\n",
    "\n",
    "# Q4. What is boosting?\n",
    "# Boosting is an ensemble technique that focuses on improving the performance of a weak learner by sequentially training multiple models, each giving more weight to the misclassified instances from the previous model. The predictions of these models are combined to make the final prediction. Boosting algorithms like AdaBoost and Gradient Boosting are popular examples.\n",
    "\n",
    "# Q5. What are the benefits of using ensemble techniques?\n",
    "# The benefits of using ensemble techniques include:\n",
    "\n",
    "# Improved accuracy and robustness: Combining multiple models often leads to better predictions.\n",
    "# Reduced overfitting: Ensembles help mitigate overfitting by smoothing out individual model errors.\n",
    "# Handling complex patterns: Ensembles can capture complex relationships in the data that single models might miss.\n",
    "# Better generalization: Ensembles perform well on new, unseen data, making them more reliable.\n",
    "# Q6. Are ensemble techniques always better than individual models?\n",
    "# While ensemble techniques often lead to improved performance, they are not guaranteed to be better than individual models in all cases. Ensembles might become complex and computationally intensive, and there's a risk of overfitting if not properly managed. In some cases, simpler models might work well and be easier to interpret.\n",
    "\n",
    "# Q7. How is the confidence interval calculated using bootstrap?\n",
    "# Bootstrap is a resampling technique used to estimate the sampling distribution of a statistic by repeatedly sampling with replacement from the original dataset. To calculate a confidence interval using bootstrap, you:\n",
    "\n",
    "# Sample with replacement from the original data to create multiple bootstrap samples.\n",
    "# Calculate the statistic of interest (mean, median, etc.) for each bootstrap sample.\n",
    "# Calculate the desired confidence interval using the distribution of bootstrap sample statistics.\n",
    "# Q8. How does bootstrap work and What are the steps involved in bootstrap?\n",
    "# Bootstrap works by resampling from the original dataset to create multiple new datasets (bootstrap samples) of the same size as the original. The steps involved in bootstrap are:\n",
    "\n",
    "# Sample with Replacement: Randomly select data points from the original dataset, allowing duplicates.\n",
    "# Calculate Statistic: Calculate the desired statistic (mean, median, etc.) on each bootstrap sample.\n",
    "# Repeat: Repeat steps 1 and 2 many times (e.g., thousands of times) to create a distribution of the statistic.\n",
    "# Calculate Confidence Interval: Use the distribution of bootstrap sample statistics to calculate the desired confidence interval.\n",
    "# Q9. Using bootstrap to estimate the 95% confidence interval for the population mean height:\n",
    "# Given the sample mean height of 15 meters, standard deviation of 2 meters, and a sample size of 50 trees, you can use the bootstrap method to estimate the 95% confidence interval for the population mean height as follows:\n",
    "\n",
    "# Sample with Replacement: Randomly select 50 heights from the sample with replacement.\n",
    "# Calculate Statistic: Calculate the mean height for each bootstrap sample.\n",
    "# Repeat: Repeat step 1 and 2 a large number of times (e.g., 10,000 times).\n",
    "# Calculate Confidence Interval: Calculate the 2.5th and 97.5th percentiles of the distribution of bootstrap sample means. These percentiles define the 95% confidence interval.\n",
    "# Here's how you can perform this in Python:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Given data\n",
    "sample_mean = 15\n",
    "sample_std = 2\n",
    "sample_size = 50\n",
    "num_bootstrap_samples = 10000\n",
    "\n",
    "# Generate bootstrap samples\n",
    "bootstrap_sample_means = []\n",
    "for _ in range(num_bootstrap_samples):\n",
    "    bootstrap_sample = np.random.normal(sample_mean, sample_std, sample_size)\n",
    "    bootstrap_sample_mean = np.mean(bootstrap_sample)\n",
    "    bootstrap_sample_means.append(bootstrap_sample_mean)\n",
    "\n",
    "# Calculate confidence interval\n",
    "confidence_interval = np.percentile(bootstrap_sample_means, [2.5, 97.5])\n",
    "\n",
    "print(\"95% Confidence Interval:\")\n",
    "print(confidence_interval)\n",
    "\n",
    "# This code uses the normal distribution assumption to generate bootstrap samples and then calculates the confidence interval using percentiles of the bootstrap sample means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367b97cc-e025-431c-880d-d91e2b9f65de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

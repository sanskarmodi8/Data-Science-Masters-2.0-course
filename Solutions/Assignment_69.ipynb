{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84037d60-49f1-420c-96f0-2157d3555363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. The K-Nearest Neighbors (KNN) algorithm is a supervised machine learning algorithm used for both classification and regression tasks. It works based on the idea that data points with similar features tend to be close to each other in the feature space. KNN classifies or predicts the target value of a data point by examining the K-nearest data points in the training dataset and making decisions based on their majority class (for classification) or averaging their values (for regression).\n",
    "\n",
    "# Q2. Choosing the value of K in KNN depends on the dataset and the problem you're trying to solve. Common methods to select the appropriate K value include:\n",
    "\n",
    "# Cross-validation: Split your dataset into training and validation sets and evaluate the model's performance for different K values.\n",
    "# The square root of the number of data points: A rule of thumb is to set K to the square root of the number of data points.\n",
    "# Experimentation: Try different K values and observe how they affect the model's performance.\n",
    "# Q3. The main difference between KNN classifier and KNN regressor is in their tasks:\n",
    "\n",
    "# KNN Classifier: It is used for classification tasks and assigns a class label to a data point based on the majority class among its K-nearest neighbors.\n",
    "# KNN Regressor: It is used for regression tasks and predicts a numerical value for a data point by averaging the values of its K-nearest neighbors.\n",
    "# Q4. The performance of KNN can be measured using various metrics, depending on whether it's used for classification or regression. Common evaluation metrics include:\n",
    "\n",
    "# Classification: Accuracy, precision, recall, F1-score, and ROC AUC.\n",
    "# Regression: Mean squared error (MSE), mean absolute error (MAE), R-squared (R2), and root mean squared error (RMSE).\n",
    "# Q5. The curse of dimensionality in KNN refers to the fact that as the number of features (dimensions) in the dataset increases, the Euclidean distance between data points becomes less meaningful. In high-dimensional spaces, all data points tend to be far apart, which can lead to decreased accuracy and efficiency of the KNN algorithm. To address this, feature selection, dimensionality reduction techniques, or other distance metrics may be used.\n",
    "\n",
    "# Q6. Handling missing values in KNN can be done by either imputing missing values or excluding data points with missing values:\n",
    "\n",
    "# Imputation: You can replace missing values with the mean, median, or mode of the respective feature or use more sophisticated imputation techniques.\n",
    "# Exclusion: You can choose to exclude data points with missing values from your analysis, but this can lead to a loss of information.\n",
    "# Q7. The choice between KNN classifier and regressor depends on the nature of the problem:\n",
    "\n",
    "# KNN Classifier is suitable for problems where the target variable is categorical and you waant to classify data points into classes or categories.\n",
    "# KNN Regressor is appropriate when the target variable is numerical, and you want to predict a continuous value.\n",
    "# Q8.\n",
    "\n",
    "# Strengths of KNN:\n",
    "\n",
    "# Simple and easy to understand.\n",
    "# No training phase, making it suitable for both online and offline learning.\n",
    "# Can handle multi-class classification and regression tasks.\n",
    "# Weaknesses of KNN:\n",
    "\n",
    "# Sensitive to the choice of K.\n",
    "# Computationally expensive for large datasets.\n",
    "# Suffers from the curse of dimensionality in high-dimensional spaces.\n",
    "# Can be influenced by noisy or irrelevant features.\n",
    "# To address these weaknesses, you can use techniques like feature selection, dimensionality reduction, and distance metric optimization.\n",
    "\n",
    "# Q9. Euclidean distance and Manhattan distance are two common distance metrics used in KNN:\n",
    "\n",
    "# Euclidean distance measures the straight-line distance between two points in Euclidean space. It is calculated as the square root of the sum of squared differences between corresponding coordinates.\n",
    "# Manhattan distance (also known as L1 distance or taxicab distance) measures the distance as the sum of the absolute differences between corresponding coordinates.\n",
    "# The choice between these distances depends on the nature of the data and the problem, with Manhattan distance being less sensitive to outliers and better suited for data with attributes on different scales.\n",
    "\n",
    "# Q10. Feature scaling is important in KNN because the algorithm relies on distance measures between data points. When features have different scales, those with larger scales can dominate the distance calculations, leading to biased results. Common methods of feature scaling include Min-Max scaling (scaling features to a specific range, usually [0, 1]) and z-score normalization (scaling features to have a mean of 0 and a standard deviation of 1). Scaling ensures that all features contribute equally to the distance calculations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

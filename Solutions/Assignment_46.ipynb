{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94f71817-860f-422f-b8aa-ca5211929113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 1\n",
    "\n",
    "# Simple Linear Regression: In simple linear regression, a single independent variable (feature) is used to predict a dependent variable (target). The relationship between the two is assumed to be linear. For example, predicting a person's weight based on their height.\n",
    "\n",
    "# Multiple Linear Regression: Multiple linear regression involves more than one independent variable. It aims to model the relationship between multiple independent variables and a dependent variable. For instance, predicting a house's price based on features like square footage, number of bedrooms, and location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "793f8ef6-a4ab-4220-92b9-ea42e0caf0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 2\n",
    "\n",
    "# Linear regression assumes:\n",
    "\n",
    "# Linearity: The relationship between independent and dependent variables is linear.\n",
    "# Independence: Residuals (differences between actual and predicted values) are independent of each other.\n",
    "# Homoscedasticity: Residuals have constant variance across all levels of independent variables.\n",
    "# Normality: Residuals are normally distributed.\n",
    "# No Multicollinearity: Independent variables are not highly correlated.\n",
    "\n",
    "# To check these assumptions, you can use residual plots, normality tests, scatter plots, and correlation matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "994bcc25-cbc2-4bb0-b3ed-16c552a6cedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 3\n",
    "\n",
    "# Interpreting Slope and Intercept:\n",
    "\n",
    "# Slope: The slope represents the change in the dependent variable for a unit change in the independent variable, holding other variables constant. A positive slope implies a positive relationship, and vice versa.\n",
    "# Intercept: The intercept is the value of the dependent variable when all independent variables are zero. It may or may not have practical significance depending on the context.\n",
    "\n",
    "# Example: In a linear regression predicting exam scores (dependent) based on hours studied (independent), the slope indicates how much an increase in hours studied affects the exam score, and the intercept is the expected score when no hours are studied.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3da40c75-a00a-4028-a254-5f112c59bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 4\n",
    "\n",
    "# Gradient descent is an optimization algorithm used in machine learning to minimize the error of a model. It works by iteratively adjusting the model's parameters (weights and biases) in the direction of the steepest descent of the cost function. This process is repeated until convergence is reached, ideally leading to the optimal set of parameters that minimize the model's error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88e75d41-98b3-4a9b-aadb-8bb9ab7a5fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 5\n",
    "\n",
    "# Multiple Linear Regression Model:\n",
    "# Multiple linear regression is a statistical method used to model the relationship between a dependent variable and two or more independent variables. It generalizes the concept of simple linear regression by allowing for multiple predictors. The model assumes a linear relationship between the independent variables and the dependent variable.\n",
    "\n",
    "# Differences from Simple Linear Regression:\n",
    "# The main difference between multiple linear regression and simple linear regression lies in the number of independent variables used in the model:\n",
    "\n",
    "# Number of Predictors:\n",
    "\n",
    "# In simple linear regression, there is only one independent variable used to predict the dependent variable \n",
    "# In multiple linear regression, there are two or more independent variables \n",
    "\n",
    "# Complexity and Interpretation:\n",
    "\n",
    "# Multiple linear regression models are more complex due to the presence of multiple predictors. Interpretation of coefficients becomes more nuanced as they represent the change in \n",
    "# y while keeping other variables constant.\n",
    "# Simple linear regression models are simpler to interpret since there's only one predictor.\n",
    "\n",
    "# Model Performance:\n",
    "\n",
    "# Multiple linear regression has the potential to capture more complex relationships between variables, as it can account for multiple influences on the dependent variable.\n",
    "# Simple linear regression might be appropriate when the relationship between x and y is relatively simple and can be well-captured by a straight line.\n",
    "\n",
    "# Assumptions and Challenges:\n",
    "\n",
    "# The assumptions of linear regression (linearity, independence, homoscedasticity, normality) apply to both simple and multiple linear regression models. However, in multiple linear regression, the risk of multicollinearity (high correlation between predictors) is higher, which can affect coefficient interpretation and stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1225aa8d-0748-41f3-a420-4f098c8237e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 6\n",
    "\n",
    "# Multicollinearity occurs when two or more independent variables are highly correlated, making it challenging to distinguish their individual effects on the dependent variable. This can lead to unstable and unreliable coefficient estimates. VIF (Variance Inflation Factor) is commonly used to detect multicollinearity. To address it, you can consider removing correlated variables or using dimensionality reduction techniques.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2ad2bc1-2cfb-4cf0-b57d-dc6907f0a9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 7\n",
    "\n",
    "# Polynomial Regression Model:\n",
    "\n",
    "# Polynomial regression is an extension of linear regression that allows for modeling non-linear relationships between variables. It involves fitting polynomial functions (curves) to the data points rather than straight lines. The main idea is to add polynomial terms (powers of the independent variable) to the equation to better capture the data's curvature.\n",
    "\n",
    "# Differences from Linear Regression:\n",
    "\n",
    "# Model Shape:\n",
    "\n",
    "# Linear regression fits a straight line to the data points.\n",
    "# Polynomial regression fits curves of higher degrees (quadratic, cubic, etc.) to the data, allowing it to capture more complex relationships.\n",
    "\n",
    "# Flexibility:\n",
    "\n",
    "# Linear regression assumes a linear relationship between variables.\n",
    "# Polynomial regression can handle both linear and non-linear relationships, offering greater flexibility to model different patterns.\n",
    "\n",
    "# Curve Fitting:\n",
    "\n",
    "# Linear regression might not accurately model data with non-linear trends.\n",
    "# Polynomial regression can capture curvatures, bends, and fluctuations in the data, making it suitable for data that doesn't follow a straight-line pattern.\n",
    "\n",
    "# Model Complexity:\n",
    "\n",
    "# Polynomial regression can become complex with higher-degree polynomials, potentially leading to overfitting.\n",
    "# Linear regression is simpler and often preferred when the relationship is close to linear.\n",
    "\n",
    "# Interpretability:\n",
    "\n",
    "# Linear regression coefficients have straightforward interpretations.\n",
    "# Polynomial regression coefficients might be less intuitive due to the non-linear relationships.\n",
    "\n",
    "# Risk of Overfitting:\n",
    "\n",
    "# Polynomial regression, especially with higher-degree polynomials, is more prone to overfitting the training data.\n",
    "# Linear regression has a lower risk of overfitting due to its simpler nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d31f6e-a661-4420-8ad5-c749a2a03b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 8\n",
    "\n",
    "# Advantages and Disadvantages of Polynomial Regression:\n",
    "\n",
    "# Advantages: Can model non-linear relationships, potentially providing better fits to certain data patterns.\n",
    "# Disadvantages: Prone to overfitting with high-degree polynomials. Interpretability can be challenging. Extrapolation can lead to unreliable predictions.\n",
    "\n",
    "# Polynomial regression is preferred when the relationship between variables is clearly non-linear, but its complexity should be balanced against the risk of overfitting. Linear regression is simpler and generally preferred when the relationship is close to linear or interpretability is important."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

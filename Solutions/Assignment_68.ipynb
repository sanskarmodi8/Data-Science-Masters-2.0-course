{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7efc3fc6-8422-4844-88aa-4d931b55a603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is Gradient Boosting Regression?\n",
    "# Gradient Boosting Regression is a machine learning technique used for regression tasks. It's an ensemble method that combines multiple weak regression models (typically decision trees) to create a strong regression model. The algorithm iteratively builds these weak learners and adjusts their predictions based on the errors of the previous learners, minimizing the overall prediction errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aabb4087-9818-4ed1-8982-d752ba4bf702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 12.1147\n",
      "R-squared: 0.0000\n"
     ]
    }
   ],
   "source": [
    "#Answer 2\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create a simple dataset\n",
    "X = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1)\n",
    "y = np.array([2, 3.8, 6, 8.2, 10, 12.1])\n",
    "\n",
    "# Define the number of trees (weak learners) and learning rate\n",
    "n_trees = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Initialize the initial prediction as the mean of target values\n",
    "initial_prediction = np.mean(y)\n",
    "predictions = np.full(y.shape, initial_prediction)\n",
    "\n",
    "# Implement gradient boosting algorithm\n",
    "for i in range(n_trees):\n",
    "    residuals = y - predictions\n",
    "    tree_prediction = np.mean(residuals)\n",
    "    predictions += learning_rate * tree_prediction\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y, predictions)\n",
    "r2 = r2_score(y, predictions)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9d62ae3-b9b3-4308-9734-b80c8ddaa771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Mean Squared Error: 12.1147\n",
      "Best R-squared: 0.0000\n",
      "Best Number of Trees: 50\n",
      "Best Learning Rate: 0.01\n"
     ]
    }
   ],
   "source": [
    "# Q3. Experiment with different hyperparameters to optimize the performance of the model:\n",
    "\n",
    "\n",
    "best_mse = float('inf')\n",
    "best_r2 = -float('inf')\n",
    "best_n_trees = None\n",
    "best_learning_rate = None\n",
    "\n",
    "for n_trees in [50, 100, 150, 200]:\n",
    "    for learning_rate in [0.01, 0.1, 0.2, 0.3]:\n",
    "        initial_prediction = np.mean(y)\n",
    "        predictions = np.full(y.shape, initial_prediction)\n",
    "\n",
    "        for i in range(n_trees):\n",
    "            residuals = y - predictions\n",
    "            tree_prediction = np.mean(residuals)\n",
    "            predictions += learning_rate * tree_prediction\n",
    "\n",
    "        mse = mean_squared_error(y, predictions)\n",
    "        r2 = r2_score(y, predictions)\n",
    "\n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best_n_trees = n_trees\n",
    "            best_learning_rate = learning_rate\n",
    "\n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "\n",
    "print(f\"Best Mean Squared Error: {best_mse:.4f}\")\n",
    "print(f\"Best R-squared: {best_r2:.4f}\")\n",
    "print(f\"Best Number of Trees: {best_n_trees}\")\n",
    "print(f\"Best Learning Rate: {best_learning_rate}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cf312e-1f5c-44a9-acfb-d3511d2f9475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What is a weak learner in Gradient Boosting?\n",
    "# A weak learner in Gradient Boosting is a simple model that performs slightly better than random guessing. It could be a decision tree with limited depth, a linear regression model, etc.\n",
    "\n",
    "# Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "# The intuition behind Gradient Boosting is to iteratively correct the errors made by previous weak learners. Each new weak learner is trained to predict the residuals (the difference between true values and current predictions) of the ensemble so far. This process reduces the error with each iteration, leading to a strong model.\n",
    "\n",
    "# Q6. How does the Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "# The Gradient Boosting algorithm builds an ensemble of weak learners in an iterative manner. At each iteration, it fits a new weak learner to the negative gradient of the loss function with respect to the current predictions. These weak learners are then added to the ensemble, and the final prediction is the sum of the individual predictions from all learners.\n",
    "\n",
    "# Q7. What are the steps involved in constructing the mathematical intuition of the Gradient Boosting algorithm?\n",
    "\n",
    "# Initialize predictions with the average target value.\n",
    "# Compute the negative gradient (residuals) of the loss function with respect to current predictions.\n",
    "# Fit a weak learner (e.g., decision tree) to predict the negative gradient.\n",
    "# Multiply the predictions of the weak learner by a learning rate and add them to the ensemble.\n",
    "# Repeat steps 2-4 iteratively for a predefined number of iterations.\n",
    "# The final prediction is the sum of predictions from all weak learners.\n",
    "# The loss function is optimized during the process, improving the overall predictive performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

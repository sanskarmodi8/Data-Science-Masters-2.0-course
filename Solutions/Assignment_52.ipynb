{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccf1e350-6a54-4ce3-a6f8-7756ea8d470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 1\n",
    "\n",
    "# Linear Regression: Linear regression is used when the target variable is continuous and the relationship between the independent variables and the target variable is assumed to be linear. It predicts a continuous outcome, typically represented by a real number.\n",
    "\n",
    "# Logistic Regression: Logistic regression is used when the target variable is categorical and binary in nature (two classes), representing a probability of belonging to a particular class. It models the probability that a given input example belongs to a specific category. The output is transformed using the logistic function (sigmoid) to ensure it falls between 0 and 1.\n",
    "\n",
    "# Example Scenario for Logistic Regression:\n",
    "# Suppose you're working on a medical project to predict whether a patient has a certain disease based on several medical test results (e.g., blood pressure, cholesterol levels, etc.). The outcome is binary: either the patient has the disease or they don't. Logistic regression would be more appropriate here, as it can model the probability of a patient having the disease based on the test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecfceda5-45b5-4bda-b0af-483c5f48fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 2\n",
    "\n",
    "# The cost function used in logistic regression is called the logistic loss (also known as the cross-entropy loss or log loss). It measures the error between the predicted probabilities generated by the logistic regression model and the true class labels. The goal is to minimize this cost function to find the optimal parameters that best fit the data.\n",
    "# The optimization is typically performed using gradient descent or its variants to minimize the cost function and find the optimal parameter values \n",
    "# Î¸ that best fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d9e2be1-abfb-4c50-ae26-e4d9e2f33f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 3\n",
    "\n",
    "# Regularization techniques (L1 and L2 regularization) add a penalty term to the cost function to prevent overfitting. Overfitting occurs when the model fits the training data too closely and performs poorly on new, unseen data. Regularization helps by discouraging the model from assigning excessively large coefficients to features.\n",
    "\n",
    "# L1 Regularization (Lasso): Adds the absolute values of the coefficients to the cost function. It encourages some coefficients to become exactly zero, effectively performing feature selection.\n",
    "# L2 Regularization (Ridge): Adds the squared values of the coefficients to the cost function. It discourages large coefficient values, effectively shrinking them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85383793-3741-4860-9d84-77aa36d33c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 4\n",
    "\n",
    "# The ROC (Receiver Operating Characteristic) curve is a graphical representation of a binary classifier's performance across different thresholds for classification. It plots the true positive rate (sensitivity) against the false positive rate (1-specificity).\n",
    "\n",
    "# The ROC curve is used to evaluate the trade-off between sensitivity and specificity, helping to choose an appropriate threshold for the classification problem. A perfect classifier would have an ROC curve that passes through the top-left corner (100% sensitivity, 100% specificity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83306b6d-f35a-4b73-bc3b-09d9dde45d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 5\n",
    "\n",
    "# Stepwise Selection: Features are added or removed iteratively based on their contribution to the model's performance.\n",
    "# L1 Regularization: As mentioned earlier, L1 regularization can lead to automatic feature selection by driving some coefficients to zero.\n",
    "# Feature Importance Methods: Techniques like Random Forest or Gradient Boosting can provide insights into feature importance and help prioritize relevant features.\n",
    "# Feature selection helps improve model performance by reducing noise and overfitting, improving model interpretability, and reducing computational complexity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf94eab2-8c4b-457f-983a-b6e79f265142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 6\n",
    "\n",
    "# Imbalanced datasets have a disproportionate number of examples from one class compared to the other. Techniques to address this include:\n",
    "\n",
    "# Resampling: Either oversampling the minority class or undersampling the majority class to balance the dataset.\n",
    "# Synthetic Data Generation: Creating new examples for the minority class using techniques like SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "# Cost-sensitive Learning: Assigning different misclassification costs to different classes during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2589dd-e233-4822-9497-9ea1b5bd82f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 7\n",
    "\n",
    "# Multicollinearity: When independent variables are highly correlated, it can affect coefficient estimates. Solutions include removing one of the correlated variables or using techniques like Principal Component Analysis (PCA).\n",
    "# Convergence Issues: Gradient descent may not converge due to poor initializations or overly complex models. Adjusting learning rates or using advanced optimization algorithms can help.\n",
    "# Outliers: Outliers can disproportionately influence the parameter estimates. Robust techniques or data preprocessing can mitigate this.\n",
    "# Non-Linearity: Logistic regression assumes a linear relationship between features and the log-odds. If the relationship is non-linear, feature transformations or using more complex models might be needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

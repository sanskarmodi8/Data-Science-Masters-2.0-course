{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a378878-d1a8-4ece-9b20-8db09d55b016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 1\n",
    "\n",
    "# R-squared, also known as the coefficient of determination, is a statistical metric used to evaluate the goodness of fit of a linear regression model. It quantifies the proportion of the variance in the dependent variable that is explained by the independent variable(s) included in the model. In other words, R-squared measures how well the chosen independent variable(s) account for the variability in the dependent variable.\n",
    "\n",
    "# Mathematically, R-squared is calculated using the following formula:\n",
    "\n",
    "# R^2 = 1 - SSres/SStot ;\n",
    "\n",
    "# where - \n",
    "\n",
    "# SSres is the sum of squared residuals, which represents the sum of the squared differences between the actual observed values and the predicted values obtained from the regression equation.\n",
    "# SStot is the total sum of squares, which measures the sum of the squared differences between the actual observed values and the mean of the dependent variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "065782e5-5f09-4fa1-a67b-ec94728e612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 2\n",
    "\n",
    "# Adjusted R-squared is a modification of the R-squared metric that takes into account the number of independent variables in the model. It penalizes the addition of unnecessary variables that might not improve the model's predictive power. Adjusted R-squared is calculated using a formula that incorporates the sample size and the number of predictors in the model. Unlike regular R-squared, adjusted R-squared can decrease if adding an irrelevant variable to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce9dff39-4a4b-4df9-93b8-8d31b521833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 3\n",
    "\n",
    "# Adjusted R-squared is more appropriate when comparing models with different numbers of predictors. It helps in identifying if adding more predictors to the model truly improves its explanatory power or if the apparent improvement is just due to adding more variables. Adjusted R-squared is especially useful when dealing with model complexity and potential overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16a3ba54-eb4e-42b1-8e55-e3e8b7f733db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 4\n",
    "\n",
    "# RMSE (Root Mean Squared Error) is the square root of the average of the squared differences between predicted and actual values. It provides a measure of the average magnitude of prediction errors.\n",
    "# MSE (Mean Squared Error) is the average of the squared differences between predicted and actual values. It's used to quantify the overall magnitude of prediction errors.\n",
    "# MAE (Mean Absolute Error) is the average of the absolute differences between predicted and actual values. It measures the average absolute magnitude of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4950d88e-19e5-4edc-9f73-84f7fa8820cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 5\n",
    "\n",
    "# Advantages:\n",
    "\n",
    "# RMSE and MSE give more weight to larger errors, making them sensitive to outliers.\n",
    "# MAE is more robust to outliers as it only considers absolute differences.\n",
    "\n",
    "# Disadvantages:\n",
    "    \n",
    "# RMSE and MSE might penalize outliers too harshly.\n",
    "# MAE doesn't consider the magnitude of errors, potentially overlooking the significance of larger errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "996810d1-e99d-4b73-b5d9-6ed8ae1f5ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 6\n",
    "\n",
    "# Lasso regularization adds a penalty based on the absolute values of coefficients to linear regression. It can set some coefficients to exactly zero, performing feature selection. It's useful for sparse models and when many features might be irrelevant. Ridge regularization uses squared coefficients and doesn't lead to exact zero coefficients. Choose Lasso when you suspect irrelevant features or need feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38584512-21b1-485b-a984-aaeb9b38ee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 7\n",
    "\n",
    "# Regularized linear models add penalty terms to the cost function that discourage large coefficient values. This prevents the model from fitting the noise in the training data, reducing overfitting. For example, in Ridge regression, the L2 regularization term penalizes the sum of squared coefficients. This helps the model generalize better to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50da82b9-b59e-4166-867c-29728b239c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 8\n",
    "\n",
    "# Limitations of Regularized Linear Models:\n",
    "\n",
    "# Regularization may result in biased estimates, as it shrinks the coefficients towards zero.\n",
    "# Choosing the right regularization parameter (e.g., lambda) can be challenging.\n",
    "# Regularization may not be suitable when the true underlying relationships in the data are complex and non-linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0533e0d-5bc5-4621-90cf-977dcb59441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 9\n",
    "\n",
    "# Both RMSE and MAE indicate the magnitude of prediction errors, but RMSE gives more weight to larger errors. In this case, you might lean towards Model B with an MAE of 8, as it indicates smaller average absolute errors. However, the choice depends on the context of the problem and the importance of different types of errors. RMSE might be more sensitive to outliers, affecting its suitability as a sole decision factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b9dcfc-d1f9-4cf6-9b85-c2626bc6116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 10\n",
    "\n",
    "# The choice between Ridge and Lasso depends on the specific problem and the characteristics of the dataset. Ridge regularization (L2) generally keeps all features but shrinks their coefficients, while Lasso (L1) can lead to feature selection by driving some coefficients to zero. In this case, you would need to consider the problem's requirement for feature selection and how the regularization affects the model's interpretability and performance. It's a trade-off between retaining all features (Ridge) and potentially selecting a subset of features (Lasso)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bad7ee-cdb1-4589-bc4c-22cb1158ee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. The main difference between the Euclidean distance metric and the Manhattan distance metric in KNN is how they calculate distance between data points:\n",
    "\n",
    "# Euclidean Distance: It measures the straight-line (as-the-crow-flies) distance between two points in a multi-dimensional space. It calculates the square root of the sum of squared differences in coordinates.\n",
    "# Manhattan Distance: Also known as L1 distance or taxicab distance, it calculates the distance as the sum of the absolute differences between the corresponding coordinates of two points.\n",
    "# The choice of distance metric can affect KNN's performance based on the distribution and characteristics of your data. Euclidean distance is sensitive to the magnitude of differences and works well when features are on similar scales. Manhattan distance is less sensitive to differences in magnitude and is suitable when features have different units or when you want to focus on the absolute differences between feature values. The choice depends on the specific problem and data.\n",
    "\n",
    "# Q2. Selecting the optimal value of K in KNN can be determined using techniques such as:\n",
    "\n",
    "# Cross-validation: Splitting your dataset into training and validation sets and evaluating the model's performance for different K values. Choose the K that gives the best performance on the validation set.\n",
    "# Grid search: Testing a range of K values and selecting the one that yields the best performance on a validation set.\n",
    "# Using domain knowledge: Sometimes, domain expertise can guide the choice of an appropriate K value.\n",
    "# Q3. The choice of distance metric can significantly affect KNN's performance:\n",
    "\n",
    "# Euclidean distance tends to work well when the data features are continuous and have similar scales.\n",
    "# Manhattan distance is suitable when you want to give equal importance to differences along all dimensions or when dealing with categorical features.\n",
    "# The choice depends on the nature of the data and the problem you're trying to solve. Experimentation and testing with different distance metrics may be necessary to determine which one works better for your specific use case.\n",
    "\n",
    "# Q4. Common hyperparameters in KNN classifiers and regressors include:\n",
    "\n",
    "# K (number of neighbors to consider)\n",
    "# Distance metric (Euclidean, Manhattan, etc.)\n",
    "# Weights (uniform or distance-based)\n",
    "# These hyperparameters can affect the model's performance. Tuning them can be done through techniques like cross-validation, grid search, or randomized search to find the best combination of hyperparameters that optimize model performance.\n",
    "\n",
    "# Q5. The size of the training set can impact KNN's performance. With a very small training set, the model might overfit, while with a very large training set, it might become computationally expensive. Techniques to optimize the size of the training set include:\n",
    "\n",
    "# Cross-validation: Use different subsets of your data for training and validation to assess how model performance changes with different training set sizes.\n",
    "# Learning curves: Plot the model's performance against different training set sizes to determine the point of diminishing returns.\n",
    "# Q6. Potential drawbacks of using KNN as a classifier or regressor include:\n",
    "\n",
    "# Computational inefficiency for large datasets.\n",
    "# Sensitivity to the choice of K and distance metric.\n",
    "# Lack of interpretability in the model.\n",
    "# To address these drawbacks, you can:\n",
    "\n",
    "# Use dimensionality reduction techniques to reduce the number of features.\n",
    "# Optimize K and distance metric through hyperparameter tuning.\n",
    "# Combine KNN with other techniques, such as weighting or feature selection.\n",
    "# Consider model explainability techniques if interpretability is essential for your application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6f90a60-8535-4ca5-b64c-0c6c18bae663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Understanding Weight Initialization\n",
    "\n",
    "# Q1: Explain the importance of weight initialization in artificial neural networks. Why is it necessary to initialize the weights carefully?\n",
    "\n",
    "# Weight initialization is crucial in neural networks because it determines the initial values of model parameters (weights). Proper initialization helps ensure that the network starts with reasonable weights, which can significantly impact training and convergence. Careful initialization is necessary to prevent issues like vanishing/exploding gradients and improve the convergence of the training process.\n",
    "\n",
    "# Q2: Describe the challenges associated with improper weight initialization. How do these issues affect model training and convergence?\n",
    "\n",
    "# Improper weight initialization can lead to challenges such as:\n",
    "# Vanishing gradients: Weights that are too small can cause gradients to become very small during backpropagation, slowing down or preventing training.\n",
    "# Exploding gradients: Weights that are too large can lead to exploding gradients, causing instability during training and divergence.\n",
    "# Slow convergence: Poor initialization can result in slower convergence and a longer time to reach an optimal solution.\n",
    "\n",
    "# Q3: Discuss the concept of variance and how it relates to weight initialization. Why is it crucial to consider the variance of weights during initialization?\n",
    "\n",
    "# Variance measures the spread or dispersion of weight values. It's essential to consider variance during initialization because it directly impacts the scale of activations and gradients in a neural network. If the initial weights have high variance, it can lead to exploding gradients, while low variance can lead to vanishing gradients. Properly initialized weights ensure moderate variance, facilitating stable and efficient training.\n",
    "\n",
    "# Part 2: Weight Initialization Techniques\n",
    "\n",
    "# Q1: Explain the concept of zero initialization. Discuss its potential limitations and when it can be appropriate to use.\n",
    "\n",
    "# Zero initialization sets all weights to zero initially. It can be appropriate for specific cases like linear regression or when the network architecture requires symmetry. However, it often leads to symmetry-breaking issues, where neurons with the same inputs learn the same features, limiting the model's expressiveness.\n",
    "\n",
    "# Q2: Describe the process of random initialization. How can random initialization be adjusted to mitigate potential issues like saturation or vanishing/exploding gradients?\n",
    "\n",
    "# Random initialization assigns small random values to weights, often sampled from a normal distribution. To mitigate issues, weights can be scaled using techniques like He initialization (scale by sqrt(2/n)) or Xavier initialization (scale by sqrt(1/n)), where 'n' is the number of input units. These techniques help control the variance of weights and avoid saturation or gradient-related problems.\n",
    "\n",
    "# Q3: Discuss the concept of Xavier/Glorot initialization. Explain how it addresses the challenges of improper weight initialization and the underlying theory behind it.\n",
    "\n",
    "# Xavier/Glorot initialization sets weights by sampling from a normal distribution with a variance that depends on both the number of input and output units. It addresses initialization challenges by ensuring that the variance remains constant across layers. The underlying theory is that if the variance remains stable, it prevents gradients from vanishing/exploding and facilitates training.\n",
    "\n",
    "# Q4: Explain the concept of He initialization. How does it differ from Xavier initialization, and when is it preferred?\n",
    "\n",
    "# He initialization, also known as He et al. initialization, sets weights by sampling from a normal distribution with a variance scaled by 2/n, where 'n' is the number of input units. It differs from Xavier initialization by using a different scaling factor. He initialization is preferred in deeper networks (e.g., deep convolutional neural networks) where it helps maintain gradient stability and promotes convergence.\n",
    "\n",
    "# Part 3: Applying Weight Initialization\n",
    "\n",
    "# see below cell\n",
    "\n",
    "# Q5: Discuss the considerations and tradeoffs when choosing the appropriate weight initialization technique for a given neural network architecture and task.\n",
    "\n",
    "# Considerations for choosing a weight initialization technique include the network architecture, activation functions, and the specific task. For instance, He initialization is often preferred for deep convolutional networks, while Xavier initialization may be suitable for shallow architectures. Tradeoffs involve balancing convergence speed, avoiding vanishing/exploding gradients, and preventing overfitting. The choice may require experimentation to find the most suitable initialization for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a8640c9-5692-4bf8-b209-55d995828a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer HeNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "Zero Initialization Test Accuracy: 0.3333\n",
      "Random Initialization Test Accuracy: 0.9333\n",
      "Xavier Initialization Test Accuracy: 0.9333\n",
      "He Initialization Test Accuracy: 0.9333\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset from scikit-learn\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define a function to create a model with a specific weight initialization technique\n",
    "def create_model(weight_initializer):\n",
    "    model = Sequential([\n",
    "        Dense(32, activation='relu', kernel_initializer=weight_initializer, input_shape=(4,)),\n",
    "        Dense(16, activation='relu', kernel_initializer=weight_initializer),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Initialize models with different weight initializations\n",
    "zero_initialized_model = create_model(tf.initializers.Zeros())\n",
    "random_initialized_model = create_model(tf.initializers.RandomNormal(mean=0.0, stddev=0.1))\n",
    "xavier_initialized_model = create_model(tf.initializers.GlorotUniform())\n",
    "he_initialized_model = create_model(tf.initializers.HeNormal())\n",
    "\n",
    "# Compile models\n",
    "models = {\n",
    "    \"Zero Initialization\": zero_initialized_model,\n",
    "    \"Random Initialization\": random_initialized_model,\n",
    "    \"Xavier Initialization\": xavier_initialized_model,\n",
    "    \"He Initialization\": he_initialized_model\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
    "    y_pred = model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred.argmax(axis=1))\n",
    "    results[name] = test_accuracy\n",
    "\n",
    "# Compare the test accuracies\n",
    "for name, accuracy in results.items():\n",
    "    print(f\"{name} Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7c800a-1673-4be8-baa9-c4917c487a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c89989a-bc5e-47df-ad02-3efd3205d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 1\n",
    "\n",
    "# using the formula of the naive bayes -- P(A/B) = ( P(B/A) . P(A) ) / P(B),\n",
    "#  => P(smoker / health plan) = ( P(health plan / smoker) . P(smoker) ) / P(health plan)\n",
    "#  => 0.40â‹…p / 0.70\n",
    " \n",
    "# P(smoker) = p which is needed to calculate exact probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce9c3492-f5b9-4b82-b7f0-7fe5e27d3783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 2\n",
    "\n",
    "# Bernoulli Naive Bayes and Multinomial Naive Bayes are both variants of the Naive Bayes algorithm used for text classification and other machine learning tasks. They are designed to handle different types of data and assumptions about feature distributions. Here's the difference between the two:\n",
    "\n",
    "# Bernoulli Naive Bayes:\n",
    "\n",
    "# Type of Features: Bernoulli Naive Bayes is used when features are binary or boolean, meaning they are either present (1) or absent (0).\n",
    "# Assumption: It assumes that features are conditionally independent given the class label.\n",
    "# Application: It's commonly used for text classification tasks where features represent the presence or absence of specific words in a document (binary bag-of-words representation).\n",
    "# Example: In email classification, each feature could represent the presence or absence of a particular word in the email.\n",
    "# Formula: It calculates the probability of a feature being present or absent given the class label.\n",
    "# Multinomial Naive Bayes:\n",
    "\n",
    "# Type of Features: Multinomial Naive Bayes is used when features are discrete and represent counts or frequencies. These could be the counts of words or tokens in a document.\n",
    "# Assumption: Similar to Bernoulli Naive Bayes, it assumes that features are conditionally independent given the class label.\n",
    "# Application: It's commonly used for text classification tasks where features represent word frequencies or counts (integer values).\n",
    "# Example: In email classification, each feature could represent the frequency of a word in the email.\n",
    "# Formula: It calculates the probability of observing a particular frequency of a feature given the class label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e22edb28-eb97-4bfe-8dee-e191cc279164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 3\n",
    "\n",
    "# Bernoulli Naive Bayes, like other variants of the Naive Bayes algorithm, is designed to work with complete data where all features are observed. It doesn't inherently handle missing values out of the box. However, there are ways to handle missing values in the context of Bernoulli Naive Bayes:\n",
    "\n",
    "# Imputation with Defaults: One simple approach is to impute missing values with a default value that makes sense in the context of binary features. For example, you could replace missing values with 0 (absence) if the feature represents the presence or absence of a certain attribute.\n",
    "\n",
    "# Use a Separate Category: If it's possible to treat missing values as a separate category, you could create an additional category for missing values. This would essentially turn the feature into a ternary feature: 0 (absence), 1 (presence), and M (missing).\n",
    "\n",
    "# Drop Missing Values: If the proportion of missing values is small and random, you might consider dropping instances with missing values from your dataset. However, this approach might result in data loss.\n",
    "\n",
    "# Advanced Imputation Techniques: If you have a significant amount of missing data, you could use more advanced imputation techniques like k-Nearest Neighbors imputation or matrix factorization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8729ae34-a732-464c-b54d-e3030297b5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 4\n",
    "\n",
    "# Yes, Gaussian Naive Bayes can be used for multi-class classification. While it's often associated with binary classification due to its name (\"Naive Bayes\"), the Gaussian Naive Bayes algorithm can be extended to handle multi-class problems as well.\n",
    "\n",
    "# In the context of multi-class classification, Gaussian Naive Bayes calculates the likelihood of each class using a Gaussian distribution (normal distribution) for each feature. The assumption is that each class's feature values follow a Gaussian distribution.\n",
    "\n",
    "# Here's how Gaussian Naive Bayes works for multi-class classification:\n",
    "\n",
    "# Training Phase:\n",
    "\n",
    "# Calculate the mean and standard deviation of each feature for each class.\n",
    "# For each class, calculate the Gaussian probability density function (PDF) for each feature's value using the class's mean and standard deviation.\n",
    "# Prediction Phase:\n",
    "\n",
    "# Given a new instance with feature values, calculate the Gaussian PDF for each class's feature distribution.\n",
    "# Combine the Gaussian PDFs using Bayes' theorem to calculate the posterior probabilities for each class.\n",
    "# Select the class with the highest posterior probability as the predicted class.\n",
    "# The Gaussian Naive Bayes algorithm is mathematically straightforward and can be effective when the features are approximately normally distributed within each class. However, keep in mind that it assumes that all features are continuous and follow a Gaussian distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fe2823c-aea8-414a-bf3a-eba5f523f157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes:\n",
      "Accuracy: 0.8839\n",
      "Precision: 0.8870\n",
      "Recall: 0.8152\n",
      "F1 Score: 0.8481\n",
      "\n",
      "Multinomial Naive Bayes:\n",
      "Accuracy: 0.7863\n",
      "Precision: 0.7393\n",
      "Recall: 0.7215\n",
      "F1 Score: 0.7283\n",
      "\n",
      "Gaussian Naive Bayes:\n",
      "Accuracy: 0.8218\n",
      "Precision: 0.7104\n",
      "Recall: 0.9570\n",
      "F1 Score: 0.8131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Answer 5\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset from the UCI repository\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "data = pd.read_csv(url, header=None)\n",
    "\n",
    "# Split the dataset into features (X) and target labels (y)\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Instantiate classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Function to evaluate classifiers and print performance metrics\n",
    "def evaluate_classifier(classifier, X, y):\n",
    "    accuracy = cross_val_score(classifier, X, y, cv=10, scoring='accuracy').mean()\n",
    "    precision = cross_val_score(classifier, X, y, cv=10, scoring='precision').mean()\n",
    "    recall = cross_val_score(classifier, X, y, cv=10, scoring='recall').mean()\n",
    "    f1 = cross_val_score(classifier, X, y, cv=10, scoring='f1').mean()\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Evaluate each classifier\n",
    "print(\"Bernoulli Naive Bayes:\")\n",
    "evaluate_classifier(bernoulli_nb, X, y)\n",
    "\n",
    "print(\"Multinomial Naive Bayes:\")\n",
    "evaluate_classifier(multinomial_nb, X, y)\n",
    "\n",
    "print(\"Gaussian Naive Bayes:\")\n",
    "evaluate_classifier(gaussian_nb, X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d9716-e359-4fff-9b9f-295a7bdfeeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on these results, we can observe that one variant of Naive Bayes outperforms the others in terms of accuracy, precision, recall, and F1 score. This could be due to the characteristics of the dataset and the assumptions made by each variant.\n",
    "\n",
    "# Conclusion:\n",
    "    \n",
    "# In conclusion, the choice of the best variant of Naive Bayes depends on the nature of the dataset and the problem you are trying to solve. Here are some observations and suggestions for future work:\n",
    "\n",
    "# Bernoulli Naive Bayes: This variant is suitable when dealing with binary features, such as the presence or absence of certain words in text classification tasks. It assumes features are conditionally independent given the class label.\n",
    "\n",
    "# Multinomial Naive Bayes: This variant is useful for discrete features that represent counts or frequencies. It's commonly used for text classification where features represent word frequencies. It also assumes feature independence.\n",
    "\n",
    "# Gaussian Naive Bayes: This variant is intended for continuous features that follow a Gaussian distribution. It might perform well if features exhibit a normal distribution within classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

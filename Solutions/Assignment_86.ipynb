{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e018e230-ba43-4d4e-ac95-332ee6a23a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. An activation function in the context of artificial neural networks is a mathematical function that determines the output of a neuron or node based on its weighted input. It introduces non-linearity into the network, allowing neural networks to model complex relationships in data.\n",
    "\n",
    "# Q2. Common types of activation functions used in neural networks include:\n",
    "\n",
    "# Sigmoid function\n",
    "# Rectified Linear Unit (ReLU)\n",
    "# Hyperbolic Tangent (tanh)\n",
    "# Softmax\n",
    "\n",
    "# Q3. Activation functions affect the training process and performance of a neural network in several ways:\n",
    "\n",
    "# They introduce non-linearity, enabling the network to learn complex mappings.\n",
    "# They help in gradient propagation during backpropagation, allowing for effective weight updates.\n",
    "# The choice of activation function can impact convergence speed and the network's ability to generalize.\n",
    "\n",
    "# Q4. The sigmoid activation function, also known as the logistic function, maps input values to a range between 0 and 1. It is defined as:\n",
    "\n",
    "# sigmoid(x) = 1 / (1 + exp(-x))\n",
    "# Advantages:\n",
    "\n",
    "# Output values are bounded between 0 and 1, which can be interpreted as probabilities.\n",
    "# Smooth and differentiable, making it suitable for gradient-based optimization.\n",
    "# Disadvantages:\n",
    "\n",
    "# Sigmoid neurons can suffer from the vanishing gradient problem, slowing down training.\n",
    "# Outputs can saturate (close to 0 or 1), leading to the vanishing gradient problem.\n",
    "\n",
    "# Q5. The Rectified Linear Unit (ReLU) activation function is defined as:\n",
    "\n",
    "# ReLU(x) = max(0, x)\n",
    "# It differs from the sigmoid function in that it is a piecewise linear function, which means it is zero for negative inputs and linear for positive inputs.\n",
    "\n",
    "# Q6. Benefits of using the ReLU activation function over sigmoid:\n",
    "\n",
    "# Address the vanishing gradient problem: ReLU does not saturate for positive inputs, allowing for faster convergence.\n",
    "# Simplicity and computational efficiency: ReLU involves simple operations, making it computationally efficient.\n",
    "# Promotes sparsity: It encourages the activation of only a subset of neurons, leading to more efficient network representations.\n",
    "\n",
    "# Q7. The \"leaky ReLU\" is a variation of the ReLU activation function that aims to address the vanishing gradient problem. Instead of being zero for negative inputs, it allows a small, non-zero gradient, typically defined as:\n",
    "\n",
    "# LeakyReLU(x) = x if x > 0, else alpha * x\n",
    "# The parameter \"alpha\" determines the slope of the negative side of the function and is usually a small positive value.\n",
    "\n",
    "# Leaky ReLU helps prevent dead neurons and allows gradients to flow through during training.\n",
    "\n",
    "# Q8. The softmax activation function is used to transform a vector of real numbers into a probability distribution. It is commonly used in the output layer of a neural network for multi-class classification tasks. Softmax normalizes the input values so that they sum to 1, making them interpretable as class probabilities.\n",
    "\n",
    "# Q9. The hyperbolic tangent (tanh) activation function is defined as:\n",
    "\n",
    "# tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    "# Compared to the sigmoid function, tanh is zero-centered (output ranges from -1 to 1). Advantages of tanh over sigmoid include stronger gradients and less susceptibility to the vanishing gradient problem. However, like sigmoid, it can still suffer from vanishing gradients for deep networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

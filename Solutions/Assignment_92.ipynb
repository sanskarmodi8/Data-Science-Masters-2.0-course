{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "251b998f-f8cb-4076-a191-ab34c3228557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Understanding Regularization\n",
    "\n",
    "# Q1: What is regularization in the context of deep learning? Why is it important?\n",
    "\n",
    "# Regularization in deep learning is a technique used to prevent overfitting by adding a penalty term to the loss function, discouraging the model from learning overly complex patterns in the training data. It is essential because deep neural networks have a high capacity to fit training data precisely, which can lead to poor generalization on unseen data.\n",
    "\n",
    "# Q2: Explain the bias-variance tradeoff and how regularization helps in addressing this tradeoff.\n",
    "\n",
    "# The bias-variance tradeoff refers to the tradeoff between a model's ability to fit training data well (low bias) and its ability to generalize to unseen data (low variance). Regularization helps address this tradeoff by adding a penalty that discourages the model from fitting the training data too closely (high bias). This encourages the model to generalize better, reducing overfitting.\n",
    "\n",
    "# Q3: Describe the concept of L1 and L2 regularization. How do they differ in terms of penalty calculation and their effects on the model?\n",
    "\n",
    "# L1 regularization adds a penalty term to the loss function based on the absolute values of the model's weights. It encourages sparse weight values, effectively selecting a subset of features.\n",
    "# L2 regularization adds a penalty term based on the square of the model's weights. It discourages large weight values but does not lead to sparse solutions. It tends to distribute the impact of all features more evenly.\n",
    "\n",
    "# Q4: Discuss the role of regularization in preventing overfitting and improving the generalization of deep learning models.\n",
    "\n",
    "# Regularization helps prevent overfitting by adding a penalty for complexity in the loss function. It encourages the model to have simpler, smoother decision boundaries, which can generalize better to unseen data. By controlling the model's capacity and discouraging extreme weight values, regularization improves generalization performance.\n",
    "\n",
    "# Part 2: Regularization Techniques\n",
    "\n",
    "# Q1: Explain Dropout regularization and how it works to reduce overfitting. Discuss the impact of Dropout on model training and inference.\n",
    "\n",
    "# Dropout is a regularization technique where random neurons are temporarily removed (dropped out) during each training iteration. This prevents specific neurons from relying too heavily on each other, reducing overfitting. During inference, dropout is typically turned off, and the model's predictions are averaged over multiple dropout-masked networks, providing more robust predictions.\n",
    "\n",
    "# Q2: Describe the concept of Early Stopping as a form of regularization. How does it help prevent overfitting during the training process?\n",
    "\n",
    "# Early Stopping is a regularization technique where training is halted when the model's performance on a validation dataset stops improving. It helps prevent overfitting by avoiding further training when the model starts fitting noise in the training data. Early Stopping ensures that the model is trained for an optimal number of epochs, reducing overfitting.\n",
    "\n",
    "# Q3: Explain the concept of Batch Normalization and its role as a form of regularization. How does Batch Normalization help in preventing overfitting?\n",
    "\n",
    "# Batch Normalization is a technique that normalizes the inputs of each layer within a mini-batch during training. It helps prevent overfitting by reducing internal covariate shift, making training more stable. Batch Normalization also acts as a regularizer by introducing slight noise to the input data, similar to Dropout, which helps prevent overfitting.\n",
    "\n",
    "# Part 3: Applying Regularization\n",
    "\n",
    "# Q1: Implement Dropout regularization in a deep learning model using a framework of your choice. Evaluate its impact on model performance and compare it with a model without Dropout.\n",
    "\n",
    "# see below cell\n",
    "\n",
    "# Q2: Discuss the considerations and tradeoffs when choosing the appropriate regularization technique for a given deep learning task.\n",
    "\n",
    "# When choosing a regularization technique, consider factors like the dataset size, model complexity, and the presence of overfitting.\n",
    "# Dropout is suitable for deep networks and large datasets but may not be needed for shallow models.\n",
    "# L1 and L2 regularization can be effective for controlling weight magnitudes and selecting important features.\n",
    "# Early Stopping is useful when you want to control training time and avoid overfitting.\n",
    "# Batch Normalization helps with training stability but may not be sufficient on its own for strong regularization.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6553f037-fc7a-43aa-812e-ff4493072f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-30 14:18:16.947904: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-30 14:18:17.508877: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-09-30 14:18:17.508933: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-09-30 14:18:17.512202: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-09-30 14:18:17.825001: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-30 14:18:17.828045: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-30 14:18:19.884935: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# Define a simple neural network model without Dropout\n",
    "def create_model_without_dropout():\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define a simple neural network model with Dropout\n",
    "def create_model_with_dropout():\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),  # Dropout layer with a 50% dropout rate\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),  # Dropout layer with a 50% dropout rate\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Function to train and evaluate a model\n",
    "def train_and_evaluate(model, train_images, train_labels, test_images, test_labels):\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(train_images, train_labels, epochs=10, validation_split=0.2, verbose=0)\n",
    "    \n",
    "    test_loss, test_accuracy = model.evaluate(test_images, test_labels, verbose=0)\n",
    "    \n",
    "    return history, test_loss, test_accuracy\n",
    "\n",
    "# Train and evaluate models with and without Dropout\n",
    "model_without_dropout = create_model_without_dropout()\n",
    "history_without_dropout, test_loss_without_dropout, test_accuracy_without_dropout = train_and_evaluate(\n",
    "    model_without_dropout, train_images, train_labels, test_images, test_labels)\n",
    "\n",
    "model_with_dropout = create_model_with_dropout()\n",
    "history_with_dropout, test_loss_with_dropout, test_accuracy_with_dropout = train_and_evaluate(\n",
    "    model_with_dropout, train_images, train_labels, test_images, test_labels)\n",
    "\n",
    "# Plot the training history for both models\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history_without_dropout.history['val_loss'], label='Without Dropout Validation Loss')\n",
    "plt.plot(history_with_dropout.history['val_loss'], label='With Dropout Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Validation Loss with and without Dropout')\n",
    "plt.show()\n",
    "\n",
    "# Print test losses and accuracies\n",
    "print(\"Without Dropout Test Loss:\", test_loss_without_dropout)\n",
    "print(\"Without Dropout Test Accuracy:\", test_accuracy_without_dropout)\n",
    "print(\"With Dropout Test Loss:\", test_loss_with_dropout)\n",
    "print(\"With Dropout Test Accuracy:\", test_accuracy_with_dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb63809-327b-4bc4-b39f-ea8e39301bef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

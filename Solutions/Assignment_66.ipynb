{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ef316ee-3fc2-47ab-9c12-cd5160032fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 1\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Assuming you have a dataset named \"X\" (features) and \"y\" (target)\n",
    "\n",
    "# # Feature selection\n",
    "# feature_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "\n",
    "# # Numerical pipeline\n",
    "# num_pipeline = Pipeline([\n",
    "#     ('imputer', SimpleImputer(strategy='mean')),\n",
    "#     ('scaler', StandardScaler())\n",
    "# ])\n",
    "\n",
    "# # Categorical pipeline\n",
    "# cat_pipeline = Pipeline([\n",
    "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#     ('encoder', OneHotEncoder())\n",
    "# ])\n",
    "\n",
    "# # Combine pipelines using ColumnTransformer\n",
    "# preprocessor = ColumnTransformer([\n",
    "#     ('num', num_pipeline, ['numerical_column_1', 'numerical_column_2', ...]),\n",
    "#     ('cat', cat_pipeline, ['categorical_column_1', 'categorical_column_2', ...])\n",
    "# ])\n",
    "\n",
    "# # Final pipeline with the preprocessor and the Random Forest Classifier\n",
    "# pipeline = Pipeline([\n",
    "#     ('feature_selector', feature_selector),\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "# ])\n",
    "\n",
    "# # Splitting the dataset into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# # Fitting the pipeline\n",
    "# pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluating the accuracy on the test dataset\n",
    "# y_pred = pipeline.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b780d145-b6d8-4ea1-9d8a-518b32844095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretation of Results and Possible Improvements:\n",
    "# The pipeline is designed to automate the feature engineering process, handle missing values, and build a Random Forest Classifier. The accuracy on the test dataset can help us evaluate the model's performance. If the accuracy is satisfactory, the pipeline can be used for predictions. Possible improvements include trying different feature selection methods, hyperparameter tuning for the Random Forest Classifier, and exploring other preprocessing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c37495da-2d6b-417c-b781-2fdd0e8f8d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 2\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# # Create classifiers\n",
    "# random_forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# logistic_regression_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# # Create the pipeline with the Voting Classifier\n",
    "# voting_pipeline = Pipeline([\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('classifier', VotingClassifier(estimators=[\n",
    "#         ('rf', random_forest_clf),\n",
    "#         ('lr', logistic_regression_clf)\n",
    "#     ], voting='soft'))  # 'soft' for probability-based voting\n",
    "# ])\n",
    "\n",
    "# # Fitting the pipeline\n",
    "# voting_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluating the accuracy on the test dataset\n",
    "# voting_accuracy = voting_pipeline.score(X_test, y_test)\n",
    "# print(f\"Voting Classifier Accuracy: {voting_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# In this pipeline, we're building a Voting Classifier that combines the predictions of a Random Forest Classifier and a Logistic Regression Classifier. The 'soft' voting scheme considers the probability of each class to make the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee05ed77-840a-4fcc-9072-d77a8d63c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretation of Results and Possible Improvements:\n",
    "# The accuracy of the Voting Classifier on the test dataset can help us evaluate its performance. If it performs well, it might indicate that combining the predictions of two different classifiers improves the overall accuracy. Possible improvements include tuning hyperparameters for both classifiers in the Voting Classifier and experimenting with different base classifiers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
